@/src/3D_avatars/Audio2Face-3D-Samples 
@/src/3D_avatars/document/article/github_com_NVIDIA_Audio2Face_3D_Samples.pdf 
@/src/3D_avatars/document/article/developer-nvidia-com-blog-nvidia-open-sources-audio2face-animation-model.pdf 

上記の資料全てを見て、このnvidiaの3Dアバターを動かす方法を模索してください
どんな環境がいるのか、どんなGPUがいるのか、pythonで動かせるのか、どんなpython環境がいるのか、動かしたものをどうチェックするのか、詳しいことを日本語で解説

上記のシステムを動かすことができるpythonプログラム、src/3D_avatars/run_Audio2Face.pyを作成してください
python run_Audio2Face.pyだけで動かせるようにしてください

現状の実行環境がかなり特殊なので、それも考慮に入れた実装にして欲しいです
現在私が操作している、結果を見たいPCはmacbookです
このプログラムを実行するのはtailscaleで接続している先のGUIなしのLinuxサーバーです
そこでの環境構築は、以下のプログラムファイルを使用しています
@/.docker/Dockerfile
@/compose.gpu.yml
@/compose.yml

dockerでlinux環境を立ち上げ、そこにpython環境を作り上げる構成です
そのサーバーにmacbookからSSH接続しているというわけです
もちろん接続先のサーバーとmacbookは異なるwifiに接続しています
私は結果の確認はこのmacbookで行いたいです
レンダリングした画像から動画を作成し、そのmp4を保存して欲しいです（大学サーバーなので通信が制限されているため）

この環境、要望でも実装可能なものを開発して欲しいです

日本語で最初にコメントアウトで使い方を説明してください
コマンドは実行せずにプログラムの作成のみ行ってください
日本語で解説